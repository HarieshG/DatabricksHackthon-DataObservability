{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399905c6-84de-4d77-9445-5a611383490c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql('select * from default.gold_fnb_sales')\n",
    "\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "\n",
    "# Filter only numeric columns\n",
    "numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "# Select only those columns\n",
    "df_numeric = df.select(numeric_cols)\n",
    "\n",
    "categorical_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "df_categorical = df.select(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ec8072-79a2-4699-a81c-e70def5ea4b0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763055245441}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ee4574-4944-40f3-b50e-138909e24d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, lit\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "# Get only numeric columns\n",
    "numeric_cols = [f.name for f in df_numeric.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "# Build expressions to count zeros in each numeric column\n",
    "zero_count_exprs = [_sum((col(c) == 0).cast(\"int\")).alias(c) for c in numeric_cols]\n",
    "\n",
    "# Aggregate and show result\n",
    "df_zeros = df_numeric.agg(*zero_count_exprs)\n",
    "df_zeros = df_zeros.select([col(c).cast(\"string\").alias(c) for c in df_zeros.columns]).withColumn('summary',lit('zeros'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5f9d9f-23fb-4169-bcd2-a90234e69bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Total row count\n",
    "total_count = df_numeric.count()\n",
    "\n",
    "# Compute null % for each column\n",
    "null_stats = (\n",
    "    df_numeric.select([\n",
    "        (F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)) / total_count * 100)\n",
    "        .cast(\"string\").alias(c)\n",
    "        for c in df_numeric.columns\n",
    "    ]).withColumn('summary',F.lit('null_%'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bddc92ea-14c4-4052-ac36-3f1e3dc51b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_describe = df_numeric.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12feaf73-18c7-4ec6-baf2-f8cacc07e829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_union = df_describe.unionByName(null_stats).unionByName(df_zeros)\n",
    "display(df_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542fb29a-b6ec-4739-ae5a-54b6936178c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Collect all rows into a dictionary\n",
    "rows = df_union.collect()\n",
    "\n",
    "# Step 2: Extract summary labels (like count, mean, etc.)\n",
    "summary_types = [row['summary'] for row in rows]\n",
    "\n",
    "# Step 3: Build a list of dicts with column names as keys\n",
    "data = []\n",
    "for col in df_union.columns:\n",
    "    if col != 'summary':\n",
    "        record = {'column_name': col}\n",
    "        for row in rows:\n",
    "            record[row['summary']] = row[col]\n",
    "        data.append(record)\n",
    "\n",
    "# Step 4: Create the transposed DataFrame\n",
    "df_transposed = spark.createDataFrame(data).select('column_name','null_%','mean','stddev','min','max','count')\n",
    "\n",
    "display(df_transposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4371dfe-a4b3-4ed0-b0f7-f6f47e2099d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Select only string/categorical columns\n",
    "categorical_cols = [f.name for f in df_categorical.schema.fields if f.dataType.simpleString() == 'string']\n",
    "\n",
    "# Get total number of rows\n",
    "total_count = df_categorical.count()\n",
    "\n",
    "# Initialize results\n",
    "result = []\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    # distinct count\n",
    "    distinct_count = df_categorical.select(col_name).distinct().count()\n",
    "    \n",
    "    # null percentage\n",
    "    null_count = df_categorical.filter(F.col(col_name).isNull() | (F.col(col_name) == \"\")).count()\n",
    "    null_pct = round((null_count / total_count) * 100, 2)\n",
    "    \n",
    "    # top value and its frequency\n",
    "    top_row = (\n",
    "        df_categorical.groupBy(col_name)\n",
    "          .count()\n",
    "          .orderBy(F.desc(\"count\"))\n",
    "          .first()\n",
    "    )\n",
    "    \n",
    "    if top_row:\n",
    "        top_value = top_row[0]\n",
    "        freq_top = top_row[1]\n",
    "    else:\n",
    "        top_value = None\n",
    "        freq_top = 0\n",
    "    \n",
    "    # append result\n",
    "    result.append((col_name, distinct_count, null_pct, distinct_count, top_value, freq_top))\n",
    "\n",
    "# create final dataframe\n",
    "profile_df = spark.createDataFrame(result, [\"column_name\", \"distinct_count\", \"null%\", \"unique\", \"top\", \"frequen_top\"])\n",
    "\n",
    "display(profile_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6eec0a-5f5b-4d58-884e-7a90a6b67e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import uuid, os\n",
    "\n",
    "df = profile_df\n",
    "CAT, SCH, TBL = \"workspace\", \"default\", \"profile_distribution\"\n",
    "FULL = f\"{CAT}.{SCH}.{TBL}\"\n",
    "\n",
    "try:\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(FULL)\n",
    "    print(f\"✅ Successfully wrote table: {FULL}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Write to {FULL} failed: {str(e).splitlines()[0]}\")\n",
    "    # Fallback: save in your user folder (always writable)\n",
    "    try:\n",
    "        user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "    except Exception:\n",
    "        user = os.environ.get(\"USER\") or \"unknown_user\"\n",
    "    safe_user = user.replace(\"@\", \"_at_\").replace(\" \", \"_\")\n",
    "    path = f\"/Users/{safe_user}/do_tool/{TBL}_{uuid.uuid4().hex}\"\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"✅ Saved Delta files to: {path}\")\n",
    "    print(f\"\\nIf you want it registered as a shared table, ask an admin to run:\\n\"\n",
    "          f\"CREATE TABLE {FULL} USING DELTA LOCATION '{path}';\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fcd586-7ab2-4ad0-bff8-59eefa35cbc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import uuid, os\n",
    "\n",
    "df = df_transposed\n",
    "CAT, SCH, TBL = \"workspace\", \"default\", \"numerical_distribution\"\n",
    "FULL = f\"{CAT}.{SCH}.{TBL}\"\n",
    "\n",
    "try:\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(FULL)\n",
    "    print(f\"✅ Successfully wrote table: {FULL}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Write to {FULL} failed: {str(e).splitlines()[0]}\")\n",
    "    # Fallback: save in your user folder (always writable)\n",
    "    try:\n",
    "        user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "    except Exception:\n",
    "        user = os.environ.get(\"USER\") or \"unknown_user\"\n",
    "    safe_user = user.replace(\"@\", \"_at_\").replace(\" \", \"_\")\n",
    "    path = f\"/Users/{safe_user}/do_tool/{TBL}_{uuid.uuid4().hex}\"\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"✅ Saved Delta files to: {path}\")\n",
    "    print(f\"\\nIf you want it registered as a shared table, ask an admin to run:\\n\"\n",
    "          f\"CREATE TABLE {FULL} USING DELTA LOCATION '{path}';\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DO_DISTRIBUTION",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
