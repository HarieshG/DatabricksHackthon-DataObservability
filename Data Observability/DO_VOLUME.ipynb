{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f124a6-c581-4f58-b4ed-0fd3a34a9851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09698bf-7d65-4cd2-b64d-52ed2bb05611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "DESCRIBE gold_fnb_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1061b0f6-02a1-4d21-9cee-b8dfa0a595a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# delete \n",
    "# FROM silver_fnb_sales\n",
    "# WHERE RETAILER = 'COSTCO'\n",
    "#   AND CATEGORY = 'Beverages'\n",
    "#   AND BRAND IN ('Red Bull','Monster Energy','C4 Energy','Tropicana','Paper Boat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0b35c1-def3-4595-a38d-c7ef4316ad02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# delete \n",
    "# FROM bronze_fnb_sales\n",
    "# WHERE RETAILER = 'COSTCO'\n",
    "#   AND CATEGORY = 'Beverages'\n",
    "#   AND BRAND IN ('Red Bull','Monster Energy','C4 Energy','Tropicana','Paper Boat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6c0f29-d42d-4e61-b0b5-0b67ad6d61fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# delete \n",
    "# FROM bronze_fnb_sales\n",
    "# WHERE RETAILER = 'CVS'\n",
    "#   AND CATEGORY = 'Beverages'\n",
    "#   AND BRAND IN ('The Cold Pressed Juicery','Paper Boat','Amul','Horlicks','Monster Energy','C4 Energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5d3fa3-fc8f-4034-b8cc-d7e84c9e8f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# delete \n",
    "# FROM silver_fnb_sales\n",
    "# WHERE RETAILER = 'CVS'\n",
    "#   AND CATEGORY = 'Dairy_Products'\n",
    "#   AND BRAND IN ('Amul Cheese','Amul Milk','Amul Masti')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab0b298-6b80-409f-b00e-2d5955406ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## # **TABLE VOLUME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d986c1-9d9b-4811-ac06-f7809c267cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Volume pillar extractor: table-level volume metrics + partition skew heuristic\n",
    "import re, math, uuid\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# CONFIG: tables to scan\n",
    "DEFAULT_DB = \"default\"\n",
    "tables = {\n",
    "    \"Bronze\": f\"{DEFAULT_DB}.bronze_fnb_sales\",\n",
    "    \"Silver\": f\"{DEFAULT_DB}.silver_fnb_sales\",\n",
    "    \"old\":   f\"{DEFAULT_DB}.gold_fnb_sales\"\n",
    "}\n",
    "# toggle counts (row_count uses df.count() and may be slow on very large tables)\n",
    "compute_counts = True\n",
    "\n",
    "# Helpers\n",
    "def describe_detail(full_table):\n",
    "    try:\n",
    "        rows = spark.sql(f\"DESCRIBE DETAIL {full_table}\").collect()\n",
    "        if rows:\n",
    "            d = rows[0].asDict()\n",
    "            # Delta fields: 'location', 'sizeInBytes', 'partitionColumns'\n",
    "            location = d.get(\"location\") or d.get(\"Location\")\n",
    "            size = d.get(\"sizeInBytes\") or d.get(\"sizeInBytes\".lower()) or None\n",
    "            partitions = d.get(\"partitionColumns\") or d.get(\"partitioncolumns\") or []\n",
    "            return {\"location\": location, \"sizeInBytes\": size, \"partitionColumns\": partitions}\n",
    "    except Exception:\n",
    "        return {\"location\": None, \"sizeInBytes\": None, \"partitionColumns\": None}\n",
    "\n",
    "def get_table_size_from_path(path):\n",
    "    # sum sizes recursively under path when DESCRIBE DETAIL didn't give sizeInBytes\n",
    "    try:\n",
    "        def recurse_sum(p):\n",
    "            try:\n",
    "                files = dbutils.fs.ls(p)\n",
    "            except Exception:\n",
    "                return 0\n",
    "            s = 0\n",
    "            for f in files:\n",
    "                if f.isDir():\n",
    "                    s += recurse_sum(f.path)\n",
    "                else:\n",
    "                    mt = getattr(f, \"size\", None) or getattr(f, \"length\", None) or getattr(f, \"modificationTime\", None)\n",
    "                    # Databricks FileInfo sometimes uses 'size' or 'length'; fallback if missing\n",
    "                    if hasattr(f, \"size\"):\n",
    "                        s += int(f.size)\n",
    "                    elif hasattr(f, \"length\"):\n",
    "                        s += int(f.length)\n",
    "                    elif getattr(f, \"modificationTime\", None):\n",
    "                        # can't derive size from mod time; skip\n",
    "                        continue\n",
    "            return s\n",
    "        total = recurse_sum(path)\n",
    "        return total if total>0 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def partition_skew_heuristic(full_table, part_cols):\n",
    "    # Use only first partition column for heuristic to keep it fast\n",
    "    if not part_cols:\n",
    "        return {\"skew_label\": \"Unknown (no partitions)\", \"max_partition_count\": None, \"median_partition_count\": None, \"ratio\": None}\n",
    "    pcol = part_cols[0]\n",
    "    try:\n",
    "        # compute counts per partition value (careful with many partitions; limit to top N unique partitions to avoid explosion)\n",
    "        counts_df = spark.sql(f\"SELECT {pcol} AS p, COUNT(1) AS cnt FROM {full_table} GROUP BY {pcol}\")\n",
    "        # collect counts as list of ints\n",
    "        counts = [row[\"cnt\"] for row in counts_df.select(\"cnt\").collect()]\n",
    "        if not counts:\n",
    "            return {\"skew_label\": \"Unknown (no data)\", \"max_partition_count\": 0, \"median_partition_count\": 0, \"ratio\": None}\n",
    "        counts_sorted = sorted(counts)\n",
    "        max_c = counts_sorted[-1]\n",
    "        # median\n",
    "        n = len(counts_sorted)\n",
    "        if n % 2 == 1:\n",
    "            median_c = counts_sorted[n//2]\n",
    "        else:\n",
    "            median_c = (counts_sorted[n//2 - 1] + counts_sorted[n//2]) / 2.0\n",
    "        # avoid zero division\n",
    "        median_safe = median_c if median_c>0 else 1\n",
    "        ratio = float(max_c) / float(median_safe)\n",
    "        if ratio > 1.5:\n",
    "            label = \"Skewed\"\n",
    "        elif ratio > 3:\n",
    "            label = \"Partially skewed\"\n",
    "        else:\n",
    "            label = \"Normal\"\n",
    "        return {\"skew_label\": label, \"max_partition_count\": int(max_c), \"median_partition_count\": float(median_c), \"ratio\": round(ratio,2)}\n",
    "    except Exception:\n",
    "        return {\"skew_label\": \"Unknown (skew computation failed)\", \"max_partition_count\": None, \"median_partition_count\": None, \"ratio\": None}\n",
    "\n",
    "# Main loop: collect metrics\n",
    "out = []\n",
    "for layer, full_table in tables.items():\n",
    "    rec = {\"layer\": layer, \"full_table\": full_table}\n",
    "    # describe detail\n",
    "    det = describe_detail(full_table)\n",
    "    rec[\"storage_path\"] = det.get(\"location\")\n",
    "    rec[\"table_size_bytes_describe\"] = det.get(\"sizeInBytes\")\n",
    "\n",
    "    # row/col counts\n",
    "    if compute_counts:\n",
    "        try:\n",
    "            df = spark.table(full_table)\n",
    "            rec[\"row_count\"] = int(df.count())\n",
    "            rec[\"col_count\"] = len(df.columns)\n",
    "        except Exception as e:\n",
    "            rec[\"row_count\"] = None\n",
    "            rec[\"col_count\"] = None\n",
    "            rec[\"notes\"] = (\"count_failed:\" + str(e)) if rec.get(\"notes\") is None else rec[\"notes\"] + \";count_failed:\" + str(e)\n",
    "    else:\n",
    "        rec[\"row_count\"] = None\n",
    "        rec[\"col_count\"] = None\n",
    "\n",
    "   \n",
    "\n",
    "    # partition skew\n",
    "    part_cols = det.get(\"partitionColumns\") or []\n",
    "    if isinstance(part_cols, str):\n",
    "        # sometimes comes as string representation\n",
    "        try:\n",
    "            part_cols = eval(part_cols)\n",
    "        except Exception:\n",
    "            part_cols = [part_cols]\n",
    "    skew = partition_skew_heuristic(full_table, part_cols)\n",
    "    rec.update(skew)\n",
    "\n",
    "    out.append(rec)\n",
    "\n",
    "# Convert to DataFrame with explicit schema\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"layer\", T.StringType(), True),\n",
    "    T.StructField(\"full_table\", T.StringType(), True),\n",
    "    T.StructField(\"storage_path\", T.StringType(), True),\n",
    "    T.StructField(\"table_size_bytes_describe\", T.LongType(), True),\n",
    "    \n",
    "    T.StructField(\"row_count\", T.LongType(), True),\n",
    "    T.StructField(\"col_count\", T.IntegerType(), True),\n",
    "    T.StructField(\"skew_label\", T.StringType(), True),\n",
    "    T.StructField(\"max_partition_count\", T.LongType(), True),\n",
    "    T.StructField(\"median_partition_count\", T.DoubleType(), True),\n",
    "    T.StructField(\"ratio\", T.DoubleType(), True),\n",
    "    \n",
    "])\n",
    "rows = []\n",
    "for r in out:\n",
    "    rows.append((\n",
    "        r.get(\"layer\"),\n",
    "        r.get(\"full_table\"),\n",
    "        r.get(\"storage_path\"),\n",
    "        r.get(\"table_size_bytes_describe\"),\n",
    "      \n",
    "        r.get(\"row_count\"),\n",
    "        r.get(\"col_count\"),\n",
    "        r.get(\"skew_label\"),\n",
    "        r.get(\"max_partition_count\"),\n",
    "        r.get(\"median_partition_count\"),\n",
    "        r.get(\"ratio\"),\n",
    "        \n",
    "    ))\n",
    "volume_metrics_df = spark.createDataFrame(rows, schema=schema)\n",
    "\n",
    "display(volume_metrics_df)\n",
    "# volume_metrics_df is the PySpark DataFrame you can write to Delta like:\n",
    "# volume_metrics_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"do_tool.volume_metrics\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd33bc9b-6d70-4f05-aafd-3166b7c41114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save 'volume_metrics' DataFrame as Delta table 'workspace.default.table_volume'\n",
    "from datetime import datetime\n",
    "import uuid, os\n",
    "\n",
    "df = volume_metrics_df\n",
    "CAT, SCH, TBL = \"workspace\", \"default\", \"table_volume\"\n",
    "FULL = f\"{CAT}.{SCH}.{TBL}\"\n",
    "\n",
    "try:\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(FULL)\n",
    "    print(f\"✅ Successfully wrote table: {FULL}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Write to {FULL} failed: {str(e).splitlines()[0]}\")\n",
    "    # Fallback: save in user folder (always writable)\n",
    "    try:\n",
    "        user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "    except Exception:\n",
    "        user = os.environ.get(\"USER\") or \"unknown_user\"\n",
    "    safe_user = user.replace(\"@\", \"_at_\").replace(\" \", \"_\")\n",
    "    path = f\"/Users/{safe_user}/do_tool/{TBL}_{uuid.uuid4().hex}\"\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"✅ Saved Delta files to: {path}\")\n",
    "    print(f\"\\nIf you want it registered as a shared table, ask an admin to run:\\n\"\n",
    "          f\"CREATE TABLE {FULL} USING DELTA LOCATION '{path}';\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe54b71-2e27-42f4-8072-7f9cc646280e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **DATA VOLUME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab620b1-270e-4d68-b4f0-261affcd9192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data Volume Pillar: compare current vs previous Delta table row counts by (RETAILER, CATEGORY)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# CONFIG: your tables and how many versions to go back\n",
    "DEFAULT_DB = \"default\"\n",
    "layers = {\n",
    "    \"BRONZE\": f\"{DEFAULT_DB}.bronze_fnb_sales\",\n",
    "    \"SILVER\": f\"{DEFAULT_DB}.silver_fnb_sales\",\n",
    "    \"GOLD\":   f\"{DEFAULT_DB}.gold_fnb_sales\"\n",
    "}\n",
    "previous_version_offset = 1   # Compare with version N-1\n",
    "\n",
    "def get_version_number(table_name):\n",
    "    try:\n",
    "        df_hist = spark.sql(f\"DESCRIBE HISTORY {table_name}\")\n",
    "        versions = df_hist.select(\"version\").orderBy(F.desc(\"version\")).collect()\n",
    "        if len(versions) < 2:\n",
    "            return None, None  # no previous version\n",
    "        curr = int(versions[0][\"version\"])\n",
    "        prev = 0\n",
    "        return curr, prev\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not read history for {table_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def get_row_counts(table_name, version):\n",
    "    \"\"\"Return (RETAILER, CATEGORY, row_count) at given version\"\"\"\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").option(\"versionAsOf\", version).table(table_name)\n",
    "        agg_df = (\n",
    "            df.filter(F.col(\"RETAILER\").isNotNull() & F.col(\"CATEGORY\").isNotNull())\n",
    "              .groupBy(\"RETAILER\", \"CATEGORY\")\n",
    "              .agg(F.count(\"*\").alias(\"row_count\"))\n",
    "        )\n",
    "        return agg_df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load version {version} for {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# collect results\n",
    "results = []\n",
    "\n",
    "for layer, full_table in layers.items():\n",
    "    curr_v, prev_v = get_version_number(full_table)\n",
    "    if curr_v is None or prev_v is None:\n",
    "        print(f\"Skipping {layer} — not enough history yet.\")\n",
    "        continue\n",
    "\n",
    "    curr_df = get_row_counts(full_table, curr_v)\n",
    "    prev_df = get_row_counts(full_table, prev_v)\n",
    "    if curr_df is None or prev_df is None:\n",
    "        continue\n",
    "\n",
    "    joined = (\n",
    "        curr_df.alias(\"curr\")\n",
    "        .join(prev_df.alias(\"prev\"),\n",
    "              on=[\"RETAILER\", \"CATEGORY\"],\n",
    "              how=\"outer\")\n",
    "        .select(\n",
    "            F.coalesce(F.col(\"curr.RETAILER\"), F.col(\"prev.RETAILER\")).alias(\"RETAILER\"),\n",
    "            F.coalesce(F.col(\"curr.CATEGORY\"), F.col(\"prev.CATEGORY\")).alias(\"CATEGORY\"),\n",
    "            F.col(\"curr.row_count\").alias(\"row_count\"),\n",
    "            F.col(\"prev.row_count\").alias(\"row_count_prev\"),\n",
    "        )\n",
    "        .withColumn(\"layer\", F.lit(layer))\n",
    "    )\n",
    "\n",
    "    joined = joined.withColumn(\n",
    "        \"difference_percent\",\n",
    "        F.when(F.col(\"row_count_prev\").isNotNull() & (F.col(\"row_count_prev\") != 0),\n",
    "               ((F.col(\"row_count\") - F.col(\"row_count_prev\")) / F.col(\"row_count_prev\") * 100)\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "\n",
    "    joined = joined.withColumn(\n",
    "        \"growth_status\",\n",
    "        F.when(F.col(\"difference_percent\") > 0, F.lit(\"Growth\"))\n",
    "         .when(F.col(\"difference_percent\") < 0, F.lit(\"Drop\"))\n",
    "         .otherwise(F.lit(\"No Change\"))\n",
    "    )\n",
    "\n",
    "    results.append(joined)\n",
    "\n",
    "# combine all layers\n",
    "if results:\n",
    "    data_volume_df = results[0]\n",
    "    for df in results[1:]:\n",
    "        data_volume_df = data_volume_df.unionByName(df, allowMissingColumns=True)\n",
    "else:\n",
    "    data_volume_df = spark.createDataFrame([], T.StructType([\n",
    "        T.StructField(\"layer\", T.StringType()),\n",
    "        T.StructField(\"RETAILER\", T.StringType()),\n",
    "        T.StructField(\"CATEGORY\", T.StringType()),\n",
    "        T.StructField(\"row_count\", T.LongType()),\n",
    "        T.StructField(\"row_count_prev\", T.LongType()),\n",
    "        T.StructField(\"difference_percent\", T.DoubleType()),\n",
    "        T.StructField(\"growth_status\", T.StringType())\n",
    "        \n",
    "    ]))\n",
    "\n",
    "display(data_volume_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9db1ea-d07c-4431-8d9e-9a9f2e16573d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save 'data_volume' DataFrame as Delta table 'workspace.default.data_volume'\n",
    "from datetime import datetime\n",
    "import uuid, os\n",
    "\n",
    "df = data_volume_df\n",
    "CAT, SCH, TBL = \"workspace\", \"default\", \"data_volume\"\n",
    "FULL = f\"{CAT}.{SCH}.{TBL}\"\n",
    "\n",
    "try:\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(FULL)\n",
    "    print(f\"✅ Successfully wrote table: {FULL}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Write to {FULL} failed: {str(e).splitlines()[0]}\")\n",
    "    # Fallback: save in your user folder (always writable)\n",
    "    try:\n",
    "        user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "    except Exception:\n",
    "        user = os.environ.get(\"USER\") or \"unknown_user\"\n",
    "    safe_user = user.replace(\"@\", \"_at_\").replace(\" \", \"_\")\n",
    "    path = f\"/Users/{safe_user}/do_tool/{TBL}_{uuid.uuid4().hex}\"\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"✅ Saved Delta files to: {path}\")\n",
    "    print(f\"\\nIf you want it registered as a shared table, ask an admin to run:\\n\"\n",
    "          f\"CREATE TABLE {FULL} USING DELTA LOCATION '{path}';\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8341882319766935,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DO_VOLUME",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
