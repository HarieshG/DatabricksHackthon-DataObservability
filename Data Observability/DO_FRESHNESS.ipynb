{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad751b1-5464-4542-93f7-31f4342613ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **FRESHNESS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dcaf738-cec6-4802-a0f0-493532dcdc47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **DATA **FRESHNESS****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efe94a63-e49c-4519-b580-f4e973bf4b03",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762956794915}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH latest_per_layer AS (\n",
    "  -- bronze\n",
    "  SELECT\n",
    "    'BRONZE' AS layer,\n",
    "    RETAILER,\n",
    "    CATEGORY,\n",
    "    MAX(DATE) AS latest_date\n",
    "  FROM `workspace`.`default`.`bronze_fnb_sales`\n",
    "  WHERE RETAILER IS NOT NULL AND CATEGORY IS NOT NULL\n",
    "  GROUP BY RETAILER, CATEGORY\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- silver\n",
    "  SELECT\n",
    "    'SILVER' AS layer,\n",
    "    RETAILER,\n",
    "    CATEGORY,\n",
    "    MAX(DATE) AS latest_date\n",
    "  FROM `workspace`.`default`.`silver_fnb_sales`\n",
    "  WHERE RETAILER IS NOT NULL AND CATEGORY IS NOT NULL\n",
    "  GROUP BY RETAILER, CATEGORY\n",
    "\n",
    "  UNION ALL\n",
    "\n",
    "  -- gold\n",
    "  SELECT\n",
    "    'GOLD' AS layer,\n",
    "    RETAILER,\n",
    "    CATEGORY,\n",
    "    MAX(DATE) AS latest_date\n",
    "  FROM `workspace`.`default`.`gold_fnb_sales`\n",
    "  WHERE RETAILER IS NOT NULL AND CATEGORY IS NOT NULL\n",
    "  GROUP BY RETAILER, CATEGORY\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  layer,\n",
    "  RETAILER,\n",
    "  CATEGORY,\n",
    "  latest_date,\n",
    "\n",
    "  \n",
    "  round( (unix_timestamp(current_timestamp()) - unix_timestamp(cast(latest_date AS timestamp))) / 3600.0 , 4) AS freshness_lag_hours,\n",
    "  round( (unix_timestamp(current_timestamp()) - unix_timestamp(cast(latest_date AS timestamp))) / 86400.0 , 4) AS freshness_lag_days,\n",
    "  round( (unix_timestamp(current_timestamp()) - unix_timestamp(cast(latest_date AS timestamp))) / (86400.0*7) , 4) AS freshness_lag_weeks,\n",
    "  CASE\n",
    "    WHEN (unix_timestamp(current_timestamp()) - unix_timestamp(cast(latest_date AS timestamp))) / 86400.0 >40  THEN 'STALE'\n",
    "    WHEN (unix_timestamp(current_timestamp()) - unix_timestamp(cast(latest_date AS timestamp))) / 86400.0 > 26 THEN 'LATE'\n",
    "    ELSE 'OK'\n",
    "  END AS health\n",
    "FROM latest_per_layer\n",
    "ORDER BY freshness_lag_days DESC, layer, RETAILER, CATEGORY;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17fd24bb-7f57-42af-a19a-e791a2fe1322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save _sqldf as a Delta table in workspace.default\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import uuid, os\n",
    "\n",
    "df = _sqldf  # rename for clarity\n",
    "df = df.withColumn(\"metric_run_id\", F.lit(str(uuid.uuid4())))\n",
    "df = df.withColumn(\"run_ts\", F.current_timestamp())\n",
    "\n",
    "CAT, SCH, TBL = \"workspace\", \"default\", \"freshness_datafreshness_metrics\"\n",
    "FULL = f\"{CAT}.{SCH}.{TBL}\"\n",
    "\n",
    "try:\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(FULL)\n",
    "    print(f\"✅ Created/overwritten table: {FULL}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Write to {FULL} failed: {str(e).splitlines()[0]}\")\n",
    "    # fallback to your user folder (always writable)\n",
    "    try:\n",
    "        user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "    except Exception:\n",
    "        user = os.environ.get(\"USER\") or \"unknown_user\"\n",
    "    safe_user = user.replace(\"@\",\"_at_\").replace(\" \",\"_\")\n",
    "    path = f\"/Users/{safe_user}/do_tool/{TBL}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"✅ Saved Delta files to: {path}\\n\\nAsk admin to register it with:\\nCREATE TABLE {FULL} USING DELTA LOCATION '{path}';\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba3164a-a18c-4fa7-bcbf-0a8fe9797a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "removal for volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a34f696-a24b-4dca-a2f2-ceeb67af1a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM workspace.default.silver_fnb_sales\n",
    "WHERE RETAILER = 'WALMART' AND CATEGORY ='Beverages' AND SEGMENT ='Fruit Juices' ; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af92cd48-4b3c-4934-91fd-45799d2e4c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **TABLE FRESHNESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9bb5a2-d941-496e-90a9-2eafa0e9e8d9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"freshness_lag_seconds\":260,\"#row_number#\":53},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762943769886}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fixed: timezone-normalized minimal version of your freshness cell\n",
    "import re\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "DEFAULT_DB = \"default\"\n",
    "tables = {\n",
    "    \"bronze\": f\"{DEFAULT_DB}.bronze_fnb_sales\",\n",
    "    \"silver\": f\"{DEFAULT_DB}.silver_fnb_sales\",\n",
    "    \"gold\":   f\"{DEFAULT_DB}.gold_fnb_sales\"\n",
    "}\n",
    "\n",
    "def to_py_dt(spark_ts):\n",
    "    if spark_ts is None:\n",
    "        return None\n",
    "    # If it's already a python datetime\n",
    "    if isinstance(spark_ts, datetime):\n",
    "        dt = spark_ts\n",
    "    else:\n",
    "        try:\n",
    "            dt = spark_ts.toPyDateTime()\n",
    "        except Exception:\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(str(spark_ts))\n",
    "            except Exception:\n",
    "                return None\n",
    "    # return dt (may be naive or aware)\n",
    "    return dt\n",
    "\n",
    "# get now from Spark and normalize to tz-aware UTC\n",
    "now = spark.sql(\"SELECT current_timestamp() as ts\").collect()[0][\"ts\"]\n",
    "now_py = to_py_dt(now)\n",
    "if now_py is None:\n",
    "    now_py = datetime.now(timezone.utc)\n",
    "elif now_py.tzinfo is None:\n",
    "    now_py = now_py.replace(tzinfo=timezone.utc)\n",
    "else:\n",
    "    now_py = now_py.astimezone(timezone.utc)\n",
    "\n",
    "def get_last_updated(full_table_name):\n",
    "    notes = []\n",
    "    # 1) DESCRIBE HISTORY\n",
    "    try:\n",
    "        hist_df = spark.sql(f\"DESCRIBE HISTORY {full_table_name} LIMIT 1\")\n",
    "        rows = hist_df.collect()\n",
    "        if rows:\n",
    "            ts = rows[0][\"timestamp\"]\n",
    "            py_ts = to_py_dt(ts)\n",
    "            if py_ts is not None:\n",
    "                # normalize to tz-aware UTC\n",
    "                if py_ts.tzinfo is None:\n",
    "                    py_ts = py_ts.replace(tzinfo=timezone.utc)\n",
    "                else:\n",
    "                    py_ts = py_ts.astimezone(timezone.utc)\n",
    "                return {\"table\": full_table_name, \"last_updated\": py_ts, \"method\": \"delta_describe_history\", \"detail\": None, \"notes\": None}\n",
    "    except Exception as e:\n",
    "        notes.append(f\"describe_history_failed:{e}\")\n",
    "\n",
    "    # 2) max timestamp-like column\n",
    "    try:\n",
    "        df = spark.table(full_table_name)\n",
    "        cand_cols = [c for c,t in df.dtypes if t.startswith(\"timestamp\") or t.startswith(\"date\")]\n",
    "        if not cand_cols:\n",
    "            cand_cols = [c for c in df.columns if re.search(r\"(updated|modified|ingest|inserted|event|ts|time|dt)\", c, re.I)]\n",
    "        best_ts = None\n",
    "        best_col = None\n",
    "        for c in cand_cols:\n",
    "            try:\n",
    "                mx = df.agg(F.max(F.col(c)).alias(\"mx\")).collect()[0][\"mx\"]\n",
    "                mx_py = to_py_dt(mx)\n",
    "                if mx_py is not None:\n",
    "                    if mx_py.tzinfo is None:\n",
    "                        mx_py = mx_py.replace(tzinfo=timezone.utc)\n",
    "                    else:\n",
    "                        mx_py = mx_py.astimezone(timezone.utc)\n",
    "                    if best_ts is None or mx_py > best_ts:\n",
    "                        best_ts = mx_py\n",
    "                        best_col = c\n",
    "            except Exception:\n",
    "                continue\n",
    "        if best_ts is not None:\n",
    "            return {\"table\": full_table_name, \"last_updated\": best_ts, \"method\": \"max_timestamp_column\", \"detail\": best_col, \"notes\": None}\n",
    "    except Exception as e:\n",
    "        notes.append(f\"max_timestamp_col_failed:{e}\")\n",
    "\n",
    "    # 3) file modification times (if available)\n",
    "    try:\n",
    "        detail_rows = spark.sql(f\"DESCRIBE DETAIL {full_table_name}\").collect()\n",
    "        if detail_rows:\n",
    "            loc = detail_rows[0].asDict().get(\"location\") or detail_rows[0].asDict().get(\"Location\")\n",
    "            if loc:\n",
    "                def latest_mod_from_path(path):\n",
    "                    try:\n",
    "                        files = dbutils.fs.ls(path)\n",
    "                    except Exception:\n",
    "                        return None\n",
    "                    max_ts = None\n",
    "                    for f in files:\n",
    "                        try:\n",
    "                            if f.isDir():\n",
    "                                child = latest_mod_from_path(f.path)\n",
    "                                if child and (max_ts is None or child > max_ts):\n",
    "                                    max_ts = child\n",
    "                            else:\n",
    "                                mt = getattr(f, \"modificationTime\", None)\n",
    "                                if mt:\n",
    "                                    dt = datetime.fromtimestamp(mt/1000.0, tz=timezone.utc)\n",
    "                                    if max_ts is None or dt > max_ts:\n",
    "                                        max_ts = dt\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                    return max_ts\n",
    "                latest = latest_mod_from_path(loc)\n",
    "                if latest:\n",
    "                    return {\"table\": full_table_name, \"last_updated\": latest, \"method\": \"file_mod_time\", \"detail\": loc, \"notes\": None}\n",
    "                else:\n",
    "                    notes.append(\"no_file_mod_time_found\")\n",
    "            else:\n",
    "                notes.append(\"no_location_in_describe_detail\")\n",
    "    except Exception as e:\n",
    "        notes.append(f\"describe_detail_failed:{e}\")\n",
    "\n",
    "    return {\"table\": full_table_name, \"last_updated\": None, \"method\": None, \"detail\": None, \"notes\": \";\".join(notes) or None}\n",
    "\n",
    "# ---------------- Replace previous freshness loop with deterministic v1 vs fixed target ----------------\n",
    "from datetime import datetime, timezone\n",
    "from zoneinfo import ZoneInfo\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Target: Friday, 14 Nov 2025 07:00 IST\n",
    "target_local = datetime(2025, 11, 14, 7, 0, 0, tzinfo=ZoneInfo(\"Asia/Kolkata\"))\n",
    "target_utc = target_local.astimezone(ZoneInfo(\"UTC\"))\n",
    "\n",
    "# health thresholds (days)\n",
    "STALE_DAYS = 4.0\n",
    "LATE_DAYS = 2.0\n",
    "\n",
    "def get_version_timestamp(full_table_name, version=1):\n",
    "    \"\"\"Return version N commit timestamp as timezone-aware UTC datetime, or None if not found.\"\"\"\n",
    "    try:\n",
    "        hist = spark.sql(f\"DESCRIBE HISTORY {full_table_name}\").select(\"version\", \"timestamp\").collect()\n",
    "        for r in hist:\n",
    "            v = r.asDict().get(\"version\")\n",
    "            if v is None:\n",
    "                continue\n",
    "            if int(v) == int(version):\n",
    "                ts = r.asDict().get(\"timestamp\")\n",
    "                return to_py_dt(ts) if to_py_dt(ts) is None else (\n",
    "                    to_py_dt(ts).replace(tzinfo=timezone.utc) if to_py_dt(ts).tzinfo is None else to_py_dt(ts).astimezone(timezone.utc)\n",
    "                )\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def classify_health(days_diff):\n",
    "    if days_diff is None:\n",
    "        return \"UNKNOWN\"\n",
    "    if days_diff > STALE_DAYS:\n",
    "        return \"STALE\"\n",
    "    if days_diff > LATE_DAYS:\n",
    "        return \"LATE\"\n",
    "    return \"OK\"\n",
    "\n",
    "rows = []\n",
    "for layer, tbl in tables.items():\n",
    "    # get version 1 timestamp (UTC)\n",
    "    v1_ts = get_version_timestamp(tbl, version=1)\n",
    "    if v1_ts is not None:\n",
    "        # ensure timezone-aware UTC\n",
    "        if v1_ts.tzinfo is None:\n",
    "            v1_ts = v1_ts.replace(tzinfo=timezone.utc)\n",
    "        else:\n",
    "            v1_ts = v1_ts.astimezone(timezone.utc)\n",
    "        # compute lag = target_utc - v1_ts\n",
    "        delta = target_utc - v1_ts\n",
    "        freshness_lag_seconds = int(delta.total_seconds())\n",
    "        freshness_lag_hours = freshness_lag_seconds / 3600.0\n",
    "        freshness_lag_days = freshness_lag_seconds / 86400.0\n",
    "        freshness_lag_weeks = freshness_lag_seconds / 604800.0\n",
    "        health = classify_health(freshness_lag_days)\n",
    "        last_used_ts = v1_ts  # version1 time used as baseline\n",
    "        detection_method = \"delta_describe_history_v1\"\n",
    "    else:\n",
    "        freshness_lag_seconds = None\n",
    "        freshness_lag_hours = None\n",
    "        freshness_lag_days = None\n",
    "        freshness_lag_weeks = None\n",
    "        health = \"UNKNOWN\"\n",
    "        last_used_ts = None\n",
    "        detection_method = None\n",
    "\n",
    "    rows.append({\n",
    "        \"layer\": layer,\n",
    "        \"table\": tbl,\n",
    "        \"last_updated\": last_used_ts,\n",
    "        \"detection_method\": detection_method,\n",
    "        \"detection_detail\": f\"compared_against_{target_local.isoformat()}_IST\",\n",
    "        \"freshness_lag_weeks\": freshness_lag_weeks,\n",
    "        \"freshness_lag_days\": freshness_lag_days,\n",
    "        \"freshness_lag_hours\": freshness_lag_hours,\n",
    "        \"health\": health,\n",
    "        \"notes\": None\n",
    "    })\n",
    "\n",
    "# Build results DataFrame (same schema as before)\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"layer\", T.StringType(), True),\n",
    "    T.StructField(\"table\", T.StringType(), True),\n",
    "    T.StructField(\"last_updated\", T.TimestampType(), True),\n",
    "    T.StructField(\"freshness_lag_weeks\", T.DoubleType(), True),\n",
    "    T.StructField(\"freshness_lag_days\", T.DoubleType(), True),\n",
    "    T.StructField(\"freshness_lag_hours\", T.DoubleType(), True),\n",
    "    T.StructField(\"health\", T.StringType(), True),\n",
    "    T.StructField(\"detection_method\", T.StringType(), True),\n",
    "    T.StructField(\"detection_detail\", T.StringType(), True),\n",
    "    T.StructField(\"notes\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "# convert python datetimes to tuples (spark will accept datetime objects for timestamp fields)\n",
    "spark_rows = []\n",
    "for r in rows:\n",
    "    spark_rows.append((\n",
    "        r[\"layer\"],\n",
    "        r[\"table\"],\n",
    "        r[\"last_updated\"],\n",
    "        r[\"freshness_lag_weeks\"],\n",
    "        r[\"freshness_lag_days\"],\n",
    "        r[\"freshness_lag_hours\"],\n",
    "        r[\"health\"],\n",
    "        r[\"detection_method\"],\n",
    "        r[\"detection_detail\"],\n",
    "        r[\"notes\"]\n",
    "    ))\n",
    "\n",
    "metrics_df = spark.createDataFrame(spark_rows, schema=schema)\n",
    "# add end_to_end (use v1 gold - v1 bronze if available)\n",
    "v1_bronze = get_version_timestamp(tables[\"bronze\"], version=1)\n",
    "v1_gold = get_version_timestamp(tables[\"gold\"], version=1)\n",
    "if v1_bronze and v1_gold:\n",
    "    e2e_seconds = int((v1_gold - v1_bronze).total_seconds())\n",
    "    e2e_days = e2e_seconds / 86400.0\n",
    "else:\n",
    "    e2e_seconds = None\n",
    "    e2e_days = None\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "metrics_df = metrics_df.withColumn(\"end_to_end_seconds\", F.lit(e2e_seconds)).withColumn(\"end_to_end_days\", F.lit(e2e_days))\n",
    "\n",
    "display(metrics_df)\n",
    "print(\"Compared version 1 timestamps against target:\", target_local.isoformat(), \" (IST)\")\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "print(\"Summary (Python view):\")\n",
    "for r in rows:\n",
    "    print(r)\n",
    "print(f\"end_to_end_seconds (gold - bronze): {e2e_seconds}, end_to_end_days: {e2e_days}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9bcd30-5901-4943-bac8-e3eb79cde111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import uuid, os\n",
    "\n",
    "df = metrics_df\n",
    "CAT, SCH, TBL = \"workspace\", \"default\", \"freshness_tablefreshness\"\n",
    "FULL = f\"{CAT}.{SCH}.{TBL}\"\n",
    "\n",
    "try:\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(FULL)\n",
    "    print(f\"✅ Successfully wrote table: {FULL}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Write to {FULL} failed: {str(e).splitlines()[0]}\")\n",
    "    # Fallback: save in user folder (always writable)\n",
    "    try:\n",
    "        user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "    except Exception:\n",
    "        user = os.environ.get(\"USER\") or \"unknown_user\"\n",
    "    safe_user = user.replace(\"@\", \"_at_\").replace(\" \", \"_\")\n",
    "    path = f\"/Users/{safe_user}/do_tool/{TBL}_{uuid.uuid4().hex}\"\n",
    "    # df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "    print(f\"✅ Saved Delta files to: {path}\")\n",
    "    print(f\"\\nIf you want it registered as a shared table, ask an admin to run:\\n\"\n",
    "          f\"CREATE TABLE {FULL} USING DELTA LOCATION '{path}';\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5764190540651306,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DO_FRESHNESS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
