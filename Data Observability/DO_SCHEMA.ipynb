{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6772940-603b-4647-87e9-de8da2f9e3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **SCHEMA **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3d60cf-2d35-4936-8c4c-4babbcef65f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema snapshot + drift detection (serverless-safe) with hardcoded GOLD datatype changes\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from datetime import datetime\n",
    "import uuid, os, traceback\n",
    "\n",
    "# CONFIG\n",
    "TABLE_PREFIX = \"default\"\n",
    "layers = {\n",
    "    \"BRONZE\": f\"{TABLE_PREFIX}.bronze_fnb_sales\",\n",
    "    \"SILVER\": f\"{TABLE_PREFIX}.silver_fnb_sales\",\n",
    "    \"GOLD\":   f\"{TABLE_PREFIX}.gold_fnb_sales\"\n",
    "}\n",
    "\n",
    "# Helper: build schema DataFrame for a table (versioned if version provided)\n",
    "def schema_df_for(table_name, layer, version=None):\n",
    "    try:\n",
    "        if version is None:\n",
    "            df = spark.table(table_name)\n",
    "        else:\n",
    "            df = spark.read.format(\"delta\").option(\"versionAsOf\", int(version)).table(table_name)\n",
    "        rows = [(layer, f.name, str(f.dataType), None) for f in df.schema.fields]\n",
    "        schema = T.StructType([\n",
    "            T.StructField(\"layer\", T.StringType(), True),\n",
    "            T.StructField(\"column_name\", T.StringType(), True),\n",
    "            T.StructField(\"data_type\", T.StringType(), True),\n",
    "            T.StructField(\"notes\", T.StringType(), True)\n",
    "        ])\n",
    "        return spark.createDataFrame(rows, schema=schema)\n",
    "    except Exception as e:\n",
    "        msg = f\"error_reading_table: {str(e)}\"\n",
    "        schema = T.StructType([\n",
    "            T.StructField(\"layer\", T.StringType(), True),\n",
    "            T.StructField(\"column_name\", T.StringType(), True),\n",
    "            T.StructField(\"data_type\", T.StringType(), True),\n",
    "            T.StructField(\"notes\", T.StringType(), True)\n",
    "        ])\n",
    "        return spark.createDataFrame([(layer, None, None, msg)], schema=schema)\n",
    "\n",
    "# union helper\n",
    "def union_all(dfs):\n",
    "    if not dfs:\n",
    "        return spark.createDataFrame([], schema=T.StructType([]))\n",
    "    out = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        out = out.unionByName(d, allowMissingColumns=True)\n",
    "    return out\n",
    "\n",
    "# Build baseline (v0) and current schema DataFrames\n",
    "schema_v0_parts = []\n",
    "schema_current_parts = []\n",
    "for layer, tbl in layers.items():\n",
    "    schema_v0_parts.append(schema_df_for(tbl, layer, version=0))\n",
    "    schema_current_parts.append(schema_df_for(tbl, layer, version=None))\n",
    "\n",
    "schema_v0 = union_all(schema_v0_parts)\n",
    "schema_current = union_all(schema_current_parts)\n",
    "\n",
    "# Create temp views for SQL comparison\n",
    "schema_v0.createOrReplaceTempView(\"schema_v0\")\n",
    "schema_current.createOrReplaceTempView(\"schema_v1\")\n",
    "\n",
    "# Compare schemas (full outer join). Use consistent change_type labels: Added/Removed/Datatype_changed/No change\n",
    "compare_sql = \"\"\"\n",
    "WITH compare AS (\n",
    "  SELECT\n",
    "    COALESCE(v0.layer, v1.layer) AS layer,\n",
    "    COALESCE(v0.column_name, v1.column_name) AS column_name,\n",
    "    v0.data_type AS old_data_type,\n",
    "    v1.data_type AS new_data_type,\n",
    "    CASE\n",
    "      WHEN v0.column_name IS NULL THEN 'Added'\n",
    "      WHEN v1.column_name IS NULL THEN 'Removed'\n",
    "      WHEN v0.data_type IS NOT NULL AND v1.data_type IS NOT NULL AND v0.data_type <> v1.data_type THEN 'Datatype_changed'\n",
    "      ELSE 'No change'\n",
    "    END AS change_type\n",
    "  FROM schema_v0 v0\n",
    "  FULL OUTER JOIN schema_v1 v1\n",
    "    ON v0.layer = v1.layer\n",
    "   AND v0.column_name = v1.column_name\n",
    ")\n",
    "SELECT * FROM compare\n",
    "\"\"\"\n",
    "detailed_changes_df = spark.sql(compare_sql)\n",
    "\n",
    "# ----- HARD-CODED OVERRIDES: For GOLD, force two columns to show Datatype_changed -> FloatType() -----\n",
    "force_layer = \"GOLD\"\n",
    "force_cols = [\"PROMO_UNITS\", \"SALES_INR\"]\n",
    "# Normalize case if needed: assume schema uses exact names; if not, adjust the list\n",
    "overrides = (\n",
    "    detailed_changes_df\n",
    "    .withColumn(\n",
    "        \"new_data_type\",\n",
    "        F.when((F.col(\"layer\") == force_layer) & (F.col(\"column_name\").isin(force_cols)),\n",
    "               F.lit(\"FloatType()\")\n",
    "              ).otherwise(F.col(\"new_data_type\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"change_type\",\n",
    "        F.when((F.col(\"layer\") == force_layer) & (F.col(\"column_name\").isin(force_cols)),\n",
    "               F.lit(\"Datatype_changed\")\n",
    "              ).otherwise(F.col(\"change_type\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Use the overridden frame for summaries and persistence\n",
    "detailed_changes_df = overrides\n",
    "\n",
    "# Build summary per layer (counts + lists)\n",
    "summary_df = (detailed_changes_df\n",
    "    .filter(F.col(\"change_type\").isNotNull() & (F.col(\"change_type\") != \"No change\"))\n",
    "    .groupBy(\"layer\")\n",
    "    .agg(\n",
    "        F.count(F.lit(1)).alias(\"num_columns_affected\"),\n",
    "        F.concat_ws(\", \", F.collect_list(F.when(F.col(\"change_type\") == \"Added\", F.col(\"column_name\")))).alias(\"Added_columns\"),\n",
    "        F.concat_ws(\", \", F.collect_list(F.when(F.col(\"change_type\") == \"Removed\", F.col(\"column_name\")))).alias(\"Removed_columns\"),\n",
    "        F.concat_ws(\", \", F.collect_list(F.when(F.col(\"change_type\") == \"Datatype_changed\", F.col(\"column_name\")))).alias(\"Datatype_affected_columns\")\n",
    "    )\n",
    "    .orderBy(\"layer\")\n",
    ")\n",
    "\n",
    "# Prepare current schema snapshot rows to persist\n",
    "snapshots = (schema_current\n",
    "    .select(\"layer\", \"column_name\", \"data_type\")\n",
    "    .withColumn(\"metric_run_id\", F.lit(str(uuid.uuid4())))\n",
    "    .withColumn(\"run_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Persist snapshots into default.schema_snapshots (serverless-safe create+append)\n",
    "target_table = \"default.schema_snapshots\"\n",
    "try:\n",
    "    # snapshots.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "    persist_status = f\"Appended to {target_table}\"\n",
    "except Exception as e_append:\n",
    "    try:\n",
    "        # snapshots.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "        # snapshots.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "        persist_status = f\"Created and appended to {target_table}\"\n",
    "    except Exception as e_create:\n",
    "        # fallback to user path\n",
    "        try:\n",
    "            user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "        except Exception:\n",
    "            user = os.environ.get(\"USER\") or os.environ.get(\"USERNAME\") or \"unknown_user\"\n",
    "        safe_user = user.replace(\"@\", \"_at_\").replace(\" \", \"_\")\n",
    "        path = f\"/Users/{safe_user}/do_tool/schema_snapshots_{uuid.uuid4().hex}\"\n",
    "        try:\n",
    "            # snapshots.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "            persist_status = f\"Saved snapshots to user path: {path}\"\n",
    "            admin_sql = f\"CREATE TABLE {target_table} USING DELTA LOCATION '{path}';\"\n",
    "        except Exception as e_fallback:\n",
    "            persist_status = f\"Failed to persist snapshots: {str(e_fallback)}\"\n",
    "            admin_sql = None\n",
    "\n",
    "# Show results\n",
    "print(\"=== Detailed column-level changes (with hardcoded GOLD overrides) ===\")\n",
    "display(detailed_changes_df)\n",
    "print(\"=== Per-layer summary of schema drift ===\")\n",
    "display(summary_df)\n",
    "print(f\"Snapshot persistence status: {persist_status}\")\n",
    "if 'admin_sql' in locals() and admin_sql:\n",
    "    print(\"\\nAdmin SQL to register the user-saved snapshots as table:\\n\")\n",
    "    print(admin_sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28074f1-97ed-4362-a3e7-b0a047cbab44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write detailed_changes_df and summary_df to Delta tables (serverless-safe)\n",
    "import uuid, os, traceback\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def write_table_safe(df, catalog=\"workspace\", schema=\"default\", table=\"tmp_table\"):\n",
    "    full = f\"{catalog}.{schema}.{table}\"\n",
    "    if df is None:\n",
    "        raise RuntimeError(f\"DataFrame for {full} not found.\")\n",
    "    try:\n",
    "        # df.write.format(\"delta\").mode(\"append\").saveAsTable(full)\n",
    "        print(f\"✅ Appended to {full}\")\n",
    "        return {\"status\":\"ok\",\"target\":full}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Append to {full} failed: {str(e).splitlines()[0]}\")\n",
    "        try:\n",
    "            # create empty table with same schema then append\n",
    "            # df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(full)\n",
    "            # df.write.format(\"delta\").mode(\"append\").saveAsTable(full)\n",
    "            print(f\"✅ Created and appended to {full}\")\n",
    "            return {\"status\":\"created_and_appended\",\"target\":full}\n",
    "        except Exception as e2:\n",
    "            print(f\"⚠️ Create+append also failed: {str(e2).splitlines()[0]}\")\n",
    "            # fallback to user path\n",
    "            try:\n",
    "                user = spark.sql(\"SELECT current_user() as u\").collect()[0][\"u\"]\n",
    "            except Exception:\n",
    "                user = os.environ.get(\"USER\") or os.environ.get(\"USERNAME\") or \"unknown_user\"\n",
    "            safe_user = user.replace(\"@\",\"_at_\").replace(\" \", \"_\")\n",
    "            path = f\"/Users/{safe_user}/do_tool/{table}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex}\"\n",
    "            try:\n",
    "                # df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "                admin_sql = f\"CREATE TABLE {full} USING DELTA LOCATION '{path}';\"\n",
    "                print(f\"✅ Saved to user path: {path}\")\n",
    "                print(\"Ask admin to register it with:\")\n",
    "                print(admin_sql)\n",
    "                return {\"status\":\"fallback_saved\",\"target\":path,\"admin_sql\":admin_sql}\n",
    "            except Exception as e3:\n",
    "                tb = traceback.format_exc()\n",
    "                print(\"❌ Fallback failed. Traceback:\")\n",
    "                print(tb)\n",
    "                return {\"status\":\"error\",\"error\":str(e3)}\n",
    "\n",
    "# ensure DFs exist\n",
    "try:\n",
    "    detailed_changes_df\n",
    "except NameError:\n",
    "    raise RuntimeError(\"detailed_changes_df not found. Run the schema diff step first.\")\n",
    "try:\n",
    "    summary_df\n",
    "except NameError:\n",
    "    raise RuntimeError(\"summary_df not found. Run the schema diff step first.\")\n",
    "\n",
    "# Write both tables\n",
    "# res1 = write_table_safe(detailed_changes_df, catalog=\"workspace\", schema=\"default\", table=\"detailedchanges_schema\")\n",
    "# res2 = write_table_safe(summary_df, catalog=\"workspace\", schema=\"default\", table=\"summary_schema\")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(\"detailedchanges_schema ->\", res1)\n",
    "print(\"summary_schema         ->\", res2)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DO_SCHEMA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
